{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Ihm8GRPFEj",
        "outputId": "f2eb9928-7290-4bc0-8307-57698db50640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W2V-CNN] Epoch 1 — Loss: 1.4960 Sent Acc: 0.5727 Asp Acc: 0.7386\n",
            "[W2V-CNN] Epoch 2 — Loss: 1.1766 Sent Acc: 0.6660 Asp Acc: 0.8317\n",
            "[W2V-CNN] Epoch 3 — Loss: 1.0754 Sent Acc: 0.6969 Asp Acc: 0.8499\n",
            "[W2V-CNN] Epoch 4 — Loss: 1.0183 Sent Acc: 0.7170 Asp Acc: 0.8576\n",
            "[W2V-CNN] Epoch 5 — Loss: 0.9797 Sent Acc: 0.7320 Asp Acc: 0.8686\n",
            "[W2V-CNN] Epoch 6 — Loss: 0.9549 Sent Acc: 0.7440 Asp Acc: 0.8690\n",
            "[W2V-CNN] Epoch 7 — Loss: 0.9305 Sent Acc: 0.7532 Asp Acc: 0.8756\n",
            "[W2V-CNN] Epoch 8 — Loss: 0.8985 Sent Acc: 0.7758 Asp Acc: 0.8781\n",
            "[W2V-CNN] Epoch 9 — Loss: 0.8471 Sent Acc: 0.7983 Asp Acc: 0.8832\n",
            "[W2V-CNN] Epoch 10 — Loss: 0.8101 Sent Acc: 0.8159 Asp Acc: 0.8909\n",
            "=== Word2Vec-CNN Sentimen ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Netral     0.7143    0.9091    0.8000       561\n",
            "     Positif     0.9876    0.8318    0.9030       767\n",
            "     Negatif     0.8219    0.7827    0.8018       672\n",
            "\n",
            "    accuracy                         0.8370      2000\n",
            "   macro avg     0.8413    0.8412    0.8350      2000\n",
            "weighted avg     0.8553    0.8370    0.8401      2000\n",
            "\n",
            "Sentiment Accuracy: 83.70%\n",
            "\n",
            "=== Word2Vec-CNN Aspek ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Pelayanan     0.9252    0.8857    0.9050       796\n",
            "    Kualitas     0.9043    0.8240    0.8623       608\n",
            "       Harga     0.8436    0.9681    0.9016       596\n",
            "\n",
            "    accuracy                         0.8915      2000\n",
            "   macro avg     0.8910    0.8926    0.8896      2000\n",
            "weighted avg     0.8945    0.8915    0.8910      2000\n",
            "\n",
            "Aspect Accuracy   : 89.15%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 2. Load data & label mapping\n",
        "df = pd.read_csv('Tokopedia_preprocessed.csv')\n",
        "sent_map   = {'Netral':0, 'Positif':1, 'Negatif':2}\n",
        "aspect_map = {'Pelayanan':0, 'Kualitas':1, 'Harga':2}\n",
        "df['sent_lbl']   = df['Sentimen'].map(sent_map)\n",
        "df['aspect_lbl'] = df['Aspek'].map(aspect_map)\n",
        "\n",
        "# 3. Tokenize untuk Word2Vec\n",
        "sentences = [s.split() for s in df['clean_content'].astype(str)]\n",
        "\n",
        "# 4. Train Word2Vec di korpus kita\n",
        "w2v = Word2Vec(sentences,\n",
        "               vector_size=100,\n",
        "               window=5,\n",
        "               min_count=1,\n",
        "               workers=4,\n",
        "               epochs=10)\n",
        "\n",
        "# 5. Bangun vocab + matriks embedding\n",
        "vocab = {'<pad>':0, '<unk>':1}\n",
        "for word in w2v.wv.index_to_key:\n",
        "    if word not in vocab:\n",
        "        vocab[word] = len(vocab)\n",
        "vocab_size = len(vocab)\n",
        "emb_dim    = w2v.vector_size\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, emb_dim), dtype=np.float32)\n",
        "# idx=1 (<unk>) random atau tetap nol\n",
        "embedding_matrix[1] = np.random.normal(size=(emb_dim,))\n",
        "for w, i in vocab.items():\n",
        "    if w in w2v.wv:\n",
        "        embedding_matrix[i] = w2v.wv[w]\n",
        "\n",
        "emb_tensor = torch.tensor(embedding_matrix)\n",
        "\n",
        "# 6. Split train/test 80:20 stratify sentimen\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=42, stratify=df['sent_lbl']\n",
        ")\n",
        "\n",
        "# 7. Dataset class\n",
        "MAX_LEN = 128\n",
        "class W2VDataset(Dataset):\n",
        "    def __init__(self, df_subset):\n",
        "        self.texts = df_subset['clean_content'].tolist()\n",
        "        self.sent  = df_subset['sent_lbl'].tolist()\n",
        "        self.asp   = df_subset['aspect_lbl'].tolist()\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i):\n",
        "        toks = self.texts[i].split()\n",
        "        idxs = [vocab.get(t,1) for t in toks]\n",
        "        if len(idxs)>=MAX_LEN:\n",
        "            idxs = idxs[:MAX_LEN]\n",
        "        else:\n",
        "            idxs += [0]*(MAX_LEN-len(idxs))\n",
        "        return (torch.tensor(idxs, dtype=torch.long),\n",
        "                torch.tensor(self.sent[i], dtype=torch.long),\n",
        "                torch.tensor(self.asp [i], dtype=torch.long))\n",
        "\n",
        "train_ds = W2VDataset(train_df)\n",
        "test_ds  = W2VDataset(test_df)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32)\n",
        "\n",
        "# 8. CNN-only model dengan Word2Vec embeddings\n",
        "class W2VCnnMulti(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(emb_tensor,\n",
        "                                                  freeze=False,\n",
        "                                                  padding_idx=0)\n",
        "        self.conv  = nn.Conv1d(in_channels=emb_dim,\n",
        "                               out_channels=300,\n",
        "                               kernel_size=5)\n",
        "        self.pool  = nn.AdaptiveMaxPool1d(1)\n",
        "        self.drop  = nn.Dropout(0.5)\n",
        "        self.fc_sent = nn.Linear(300, 3)\n",
        "        self.fc_asp  = nn.Linear(300, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, MAX_LEN)\n",
        "        e = self.embed(x)           # (B, MAX_LEN, emb_dim)\n",
        "        e = e.permute(0,2,1)        # (B, emb_dim, MAX_LEN)\n",
        "        c = torch.relu(self.conv(e))# (B,300,MAX_LEN-4)\n",
        "        p = self.pool(c).squeeze(-1)# (B,300)\n",
        "        d = self.drop(p)\n",
        "        return self.fc_sent(d), self.fc_asp(d)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_w2v = W2VCnnMulti().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_w2v.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "\n",
        "# 9. Training (10 epochs)\n",
        "for epoch in range(1,11):\n",
        "    model_w2v.train()\n",
        "    tot_loss=0; corr_s=0; corr_a=0\n",
        "    for ids, sl, al in train_loader:\n",
        "        ids, sl, al = ids.to(device), sl.to(device), al.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out_s, out_a = model_w2v(ids)\n",
        "        loss = criterion(out_s, sl) + criterion(out_a, al)\n",
        "        loss.backward(); optimizer.step()\n",
        "        tot_loss += loss.item()*ids.size(0)\n",
        "        corr_s += (out_s.argmax(1)==sl).sum().item()\n",
        "        corr_a += (out_a.argmax(1)==al).sum().item()\n",
        "    n=len(train_ds)\n",
        "    print(f\"[W2V-CNN] Epoch {epoch} — Loss: {tot_loss/n:.4f} \"\n",
        "          f\"Sent Acc: {corr_s/n:.4f} Asp Acc: {corr_a/n:.4f}\")\n",
        "\n",
        "# 10. Evaluasi pada 20% test set\n",
        "model_w2v.eval()\n",
        "all_sp,all_st,all_ap,all_at = [],[],[],[]\n",
        "with torch.no_grad():\n",
        "    for ids, sl, al in test_loader:\n",
        "        ids = ids.to(device)\n",
        "        ps, pa = model_w2v(ids)\n",
        "        all_sp += ps.argmax(1).cpu().tolist()\n",
        "        all_ap += pa.argmax(1).cpu().tolist()\n",
        "        all_st += sl.tolist()\n",
        "        all_at += al.tolist()\n",
        "\n",
        "print(\"=== Word2Vec-CNN Sentimen ===\")\n",
        "print(classification_report(all_st, all_sp,\n",
        "                            target_names=['Netral','Positif','Negatif'],\n",
        "                            digits=4))\n",
        "print(f\"Sentiment Accuracy: {accuracy_score(all_st, all_sp)*100:.2f}%\\n\")\n",
        "\n",
        "print(\"=== Word2Vec-CNN Aspek ===\")\n",
        "print(classification_report(all_at, all_ap,\n",
        "                            target_names=['Pelayanan','Kualitas','Harga'],\n",
        "                            digits=4))\n",
        "print(f\"Aspect Accuracy   : {accuracy_score(all_at, all_ap)*100:.2f}%\")\n"
      ]
    }
  ]
}